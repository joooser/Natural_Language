{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "c4_texts_vocab_statistics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joooser/Natural_Language/blob/main/c4_texts_vocab_statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRi6kHk8ECaT"
      },
      "source": [
        "# Configuración inicial\n",
        "\n",
        "* En este notebook vamos a descargar el dataset `book` que contiene muchos libros en ingles en formato de texto tokenizado, de los cuales vamos a hacer procesamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLeLwV_ND0Pq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f8277dc-a1ad-4b09-8845-10c38c08b4da"
      },
      "source": [
        "import nltk \n",
        "nltk.download('book')\n",
        "from nltk.book import *\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEFQ7N_OF3W_"
      },
      "source": [
        "# Analizando un texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3y6FxQuF7Ts"
      },
      "source": [
        "# escogemos text1 que es el famoso libro Moby Dick\n",
        "text1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFlDai-3Im-R"
      },
      "source": [
        "# Vemos que el texto ya viene tokenizado incluyendo caracteres especiales ....\n",
        "text1.tokens[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO4R4EgzIAuK"
      },
      "source": [
        "# ¿Cuantos tokens tiene el libro?\n",
        "len(text1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm95nIKgJZ0F"
      },
      "source": [
        "## Medida de riqueza lexica en un texto: \n",
        "$$ R_l = \\frac{\\text{total de palabras únicas}}{\\text{total de palabras}} = \\frac{\\text{longitud del vocabulario}}{\\text{longitud del texto}}$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3I0r11xIvpz"
      },
      "source": [
        "# Primero realizamos la construcción de un vocabulario (identificamos las palabras unicas que hay en el libro)\n",
        "# https://docs.python.org/2/library/sets.html\n",
        "vocabulario = sorted(set(text1))\n",
        "vocabulario[1000:1050]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIlPOlLdI3wb"
      },
      "source": [
        "# luego definimos la medida de riqueza léxica:\n",
        "rl = len(set(text1))/len(text1)\n",
        "print(rl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KBxRjHQJ2d7"
      },
      "source": [
        "# podemos definir funciones en python para estas medidas léxicas:\n",
        "def riqueza_lexica(texto):\n",
        "  return len(set(texto))/len(texto)\n",
        "\n",
        "def porcentaje_palabra(palabra, texto):\n",
        "  return 100*texto.count(palabra)/len(texto)\n",
        "\n",
        "riqueza_lexica(text1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHzRScBFNHbY"
      },
      "source": [
        "# podemos calcular el porcentaje de texto que ocupa una palabra en una cadena larga de texto.\n",
        "porcentaje_palabra('monster', text1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLvrh39wPRBz"
      },
      "source": [
        "# y calcular en conteo el numero de veces que aparece:\n",
        "text1.count('monster')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-CMwKBzMGmK"
      },
      "source": [
        "# Estadistica del Lenguaje\n",
        "\n",
        "Los cálculos estadísticos más simples que se pueden efectuar sobre un texto o un corpus son los relacionados con frecuencia de aparición de palabras.\n",
        "\n",
        "* Podemos construir un diccionario en Python donde las llaves sean las palabras y los valores sean las frecuencias de ocurrencias de esas palabras.\n",
        "\n",
        "* ejemplo `dic = {'monster': 49 ,  'boat': 54,  ...}`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEGgKnFeKd5v"
      },
      "source": [
        "# METODO NO recomendable para conjuntos muy grandes\n",
        "dic = {}\n",
        "for palabra in set(text1):\n",
        "  #dic[palabra] = porcentaje_palabra(palabra, text1)\n",
        "  dic[palabra] = text1.count(palabra)\n",
        "dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eWzHWyAy-5B"
      },
      "source": [
        "## Método FreqDist de NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu8q9plGNnA_"
      },
      "source": [
        "# NLTK tiene un metodo muy eficiente\n",
        "fdist = FreqDist(text1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPGzPWe2OmJf"
      },
      "source": [
        "fdist.most_common(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPCV9UqyOrpt"
      },
      "source": [
        "fdist.plot(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V95L5wo4OzAJ"
      },
      "source": [
        "fdist['monster']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIv6TflAzEyi"
      },
      "source": [
        "## Distribuciones sobre contenido con filtro-fino\n",
        "\n",
        "* Como vimos en la sección anterior, los tokens más frecuentes en un texto no son necesariamente las palabras que mas informacion nos arrojan sobre el contenido del mismo. \n",
        "* Por ello, es mejor filtrar y construir distribuciones de frecuencia que no consideren signos de puntuación o caracteres especiales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3mUefGxPN9p"
      },
      "source": [
        "# Tal vez sea más interesante estudiar palabras que tengan una longitud minima especifica\n",
        "long_words = [palabra for palabra in text1 if len(palabra)>5]\n",
        "vocabulario_filtrado = sorted(set(long_words))\n",
        "vocabulario_filtrado[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKRWasT60Dmm"
      },
      "source": [
        "Podemos ahora construir una lista de tuplas, basados en los conteos ya pre-calculados de la función `FreqDist`, así:\n",
        "\n",
        "`fdist_filtrado = [('account', 2), ('additional', 32), ('advancing', 5), ...]`\n",
        "\n",
        "Luego podemos convertir el objeto creado a formato de `np.array`de **Numpy** para poder hacer un sort por palabras mas frecuentes.\n",
        "\n",
        "`fdist_filtrado = np.array(fdist_filtrado, *args)`\n",
        "\n",
        "`np.sort(fdist_filtrado, *args) = [('account', 2), ('advancing', 5), ('additional', 32), ...]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW68CjK2PuDd"
      },
      "source": [
        "palabras_interesantes = [(palabra, fdist[palabra]) for palabra in set(text1) if len(palabra)>5 and fdist[palabra]>10]\n",
        "dtypes = [('word', 'S10'), ('frequency', int)]\n",
        "palabras_interesantes = np.array(palabras_interesantes, dtype=dtypes)\n",
        "palabras_ordenadas = np.sort(palabras_interesantes, order = 'frequency')\n",
        "palabras_ordenadas[-20:] # top de palabras mas frecuentes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXoOpKx5Qzhh"
      },
      "source": [
        "# Construccion de un grafico de barras customizado con Matplotlib\n",
        "# https://matplotlib.org/3.1.1/gallery/ticks_and_spines/custom_ticker1.html#sphx-glr-gallery-ticks-and-spines-custom-ticker1-py\n",
        "top_words = 20\n",
        "x = np.arange(len(palabras_ordenadas[-top_words:]))\n",
        "y = [freq[1] for freq in palabras_ordenadas[-top_words:]]\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(x, y)\n",
        "plt.xticks(x, [str(freq[0]) for freq in palabras_ordenadas[-top_words:]], rotation = 'vertical')\n",
        "plt.tick_params(labelsize = 'xx-large')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmH1OjrOX49h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}